# THIS FILE IS GENERATED BY https://github.com/kubermatic/kubermatic/blob/master/api/hack/sync-charts.sh
# ====== kubermatic ======
kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  # The prefix for monitoring annotations in the user cluster. Default: monitoring.kubermatic.io -> monitoring.kubermatic.io/scrape, monitoring.kubermatic.io/path
  monitoringScrapeAnnotationPrefix: ""

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
      helmVersion: "v2.11.0"

  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"

  controller:
    # Available feature gates:
    # - OpenIDAuthPlugin
    #   If enabled configures the flags on the API server to use OAuth2 identity providers.
    # - VerticalPodAutoscaler
    #   If enabled the cluster-controller will enable the VerticalPodAutoscaler for all control plane components
    # - EtcdDataCorruptionChecks
    #   If enabled the all etcd clusters will be started with --experimental-initial-corrupt-check=true --experimental-corrupt-check-time=10m
    # For example:
    # featureGates: "OpenIDAuthPlugin=true,VerticalPodAutoscaler=true"
    featureGates: ""
    datacenterName: ""
    # Specifies the NodePort range for customer clusters - this must match the NodePort range of the seed cluster.
    nodeportRange: "30000-32767"
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.10.2"
      pullPolicy: "IfNotPresent"
    addons:
      kubernetes:
        # list of Addons to install into every user-cluster. All need to exist in the addons image
        defaultAddons:
        - canal
        - dashboard
        - dns
        - kube-proxy
        - openvpn
        - rbac
        - kubelet-configmap
        - default-storage-class
        - node-exporter
        image:
          repository: "quay.io/kubermatic/addons"
          tag: "v0.2.8-1"
          pullPolicy: "IfNotPresent"
      openshift:
        # list of Addons to install into every user-cluster. All need to exist in the addons image
        defaultAddons:
        - networking
        - openvpn
        - rbac
        image:
          repository: "quay.io/kubermatic/openshift-addons"
          tag: "v0.8"
          pullPolicy: "IfNotPresent"
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: controller-manager
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  api:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.10.2"
      pullPolicy: "IfNotPresent"
    resources:
      requests:
        cpu: 100m
        memory: 64Mi
      limits:
        cpu: 250m
        memory: 128Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-api
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  ui:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/ui-v2"
      tag: "v1.2.2"
      pullPolicy: "IfNotPresent"
    config: |
      {
        "default_node_count": 3,
        "share_kubeconfig": false,
        "show_demo_info": false,
        "show_terms_of_service": false,
        "cleanup_cluster": false,
        "custom_links": []
      }
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 32Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-ui
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  masterController:
    replicas: 1
    image:
      repository: quay.io/kubermatic/api
      tag: "v2.10.2"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 256Mi
    affinity: {}
    nodeSelector: {}
    tolerations: []

  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup

  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY

  clusterNamespacePrometheus: {}
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning

  vpa:
    updater:
      image:
        repository: k8s.gcr.io/vpa-updater
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    recommender:
      image:
        repository: k8s.gcr.io/vpa-recommender
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 500Mi
        limits:
          cpu: 200m
          memory: 3000Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    admissioncontroller:
      image:
        repository: k8s.gcr.io/vpa-admission-controller
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

# ====== cert-manager ======
certManager:
  controller:
    image:
      repository: quay.io/jetstack/cert-manager-controller
      tag: v0.7.0
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 10m
        memory: 30Mi
      limits:
        cpu: 30m
        memory: 50Mi

    affinity: {}
    nodeSelector: {}
    tolerations: []

  webhook:
    image:
      repository: quay.io/jetstack/cert-manager-webhook
      tag: v0.7.0
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 10m
        memory: 30Mi
      limits:
        cpu: 30m
        memory: 30Mi

    affinity: {}
    nodeSelector: {}
    tolerations: []

    # If true, the apiserver's cabundle will be automatically injected into the
    # webhook's ValidatingWebhookConfiguration resource by the CA injector.
    # If you're running on Kubernetes 1.11+, you can set this to false already.
    # see https://github.com/kubernetes/kubernetes/pull/62649
    injectAPIServerCA: false

  cainjector:
    image:
      repository: quay.io/jetstack/cert-manager-cainjector
      tag: v0.7.0
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 128Mi

    nodeSelector: {}
    affinity: {}
    tolerations: []

  securityContext:
    enabled: false
    fsGroup: 1001
    runAsUser: 1001

  ingressShim: {}
    # defaultIssuerName: ""
    # defaultIssuerKind: ""
    # defaultACMEChallengeType: ""
    # defaultACMEDNS01ChallengeProvider: ""

# ====== certs ======
certificates:
  domains:
  - "kubermatic.example.com"
  - "grafana.kubermatic.example.com"
  - "prometheus.kubermatic.example.com"
  - "alertmanager.kubermatic.example.com"
  - "kibana.kubermatic.example.com"
  issuer:
    email: dev@loodse.com

# ====== nginx-ingress-controller ======
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3
  image:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.24.1
  config: {}
#    load-balance: "least_conn"
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  tolerations:
  - key: only_critical
    operator: Equal
    value: "true"
    effect: NoSchedule

  # set this to true to automatically add these tolerations
  # to make nginx run on master nodes:
  #   - { key: dedicated, operator: Equal, value: master, effect: NoSchedule }
  #   - { key: node-role.kubernetes.io/master, effect: NoSchedule }
  ignoreMasterTaint: false

# ====== nodeport-proxy ======
nodePortPoxy:
  replicas: 3
  envoy:
    image:
      repository: "envoyproxy/envoy-alpine"
      tag: "v1.10.0"
  image:
    repository: "quay.io/kubermatic/nodeport-proxy"
    tag: "v2.0.0"

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: envoy
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  resources:
    envoy:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 20m
        memory: 64Mi
    envoyManager:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 20m
        memory: 48Mi
    lbUpdater:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 10m
        memory: 32Mi

  lbUpdater:
    nodeSelector: {}
    affinity: {}
    tolerations: []

# ====== oauth ======
dex:
  image:
    repository: "quay.io/dexidp/dex"
    tag: "v2.15.0"
  replicas: 1
  ingress:
    host: ""
    path: "/dex"
  expiry:
    signingKeys: "6h"
    idTokens: "24h"
#  connectors:
#  - type: github
#    id: github
#    name: GitHub
#    config:
#      clientID: some-client-id
#      clientSecret: some-client-secret
#      redirectURI: https://dev.kubermatic.io/dex/callback
#      orgs:
#      - name: kubermatic
#
#  clients:
#  - id: kubermatic
#    name: Kubermatic
#    secret: very-secret
#    RedirectURIs:
#    - http://localhost:8000
#    - https://dev.kubermatic.io
#    - http://localhost:8000/projects
#    - https://dev.kubermatic.io/projects
#
#  staticPasswordLogins:
#  - email: "admin@example.com"
#    # bcrypt hash of the string "password"
#    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
#    username: "admin"
#    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 50m
      memory: 200Mi

  nodeSelector: {}
  affinity: {}
  tolerations: []

# ====== minio ======
minio:
  image:
    repository: minio/minio
    tag: RELEASE.2019-04-09T01-22-30Z
  storeSize: 100Gi
  credentials:
    accessKey: wtupllWfpMg414ZM5YkzZiUmgjh1vZdk
    secretKey: r89xkN9JvHJQppb5v7SEfkNkiC1vDcMySQFKxg6uDkE3gZfCeB7ZBfECyUOTywym

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.0.0-2

  # If your cluster does not have a default storage class,
  # you can specify the class to use for Minio. Note that
  # you cannot change this later on without purging the
  # chart and losing data.
  #storageClass: hdd

  resources:
    minio:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 512Mi
    backup:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 500m
        memory: 1500Mi

  nodeSelector: {}
  affinity: {}
  tolerations: []

# ====== iap ======
iap:
  deployments:
    # alertmanager:
    #   name: alertmanager
    #   client_id: alertmanager
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   ## example configuration allowing access only to the mygroup from mygithuborg organization
    #     scopes:
    #     - "groups"
    #     resources:
    #     - uri: "/*"
    #       groups:
    #       - "mygithuborg:mygroup"
    #   upstream_service: alertmanager-kubermatic.monitoring.svc.cluster.local
    #   upstream_port: 9093
    #   ingress:
    #     host: "alertmanager.kubermatic.tld"
    #     annotations: {}
    # grafana:
    #   name: grafana
    #   client_id: grafana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: grafana.monitoring.svc.cluster.local
    #   upstream_port: 3000
    #   ingress:
    #     host: "grafana.kubermatic.tld"
    #     annotations: {}
    # kibana:
    #   name: kibana
    #   client_id: kibana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: kibana.logging.svc.cluster.local
    #   upstream_port: 5601
    #   ingress:
    #     host: "kibana.kubermatic.tld"
    #     annotations: {}
    # prometheus:
    #   name: prometheus
    #   client_id: prometheus
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: prometheus-kubermatic.monitoring.svc.cluster.local
    #   upstream_port: 9090
    #   ingress:
    #     host: "prometheus.kubermatic.tld"
    #     annotations:
    #       ingress.kubernetes.io/upstream-hash-by: "ip_hash" ## needed for prometheus federations

  discovery_url: https://kubermatic.tld/dex/.well-known/openid-configuration
  port: 3000

  image:
    repository: quay.io/gambol99/keycloak-proxy
    tag: v2.3.0
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 10m
      memory: 25Mi
    limits:
      cpu: 100m
      memory: 50Mi

  affinity: {}
  nodeSelector: {}
  tolerations: []


# ========================
# ====== Monitoring ======
# ========================

# ====== alertmanager ======
alertmanager:
  version: v0.16.2
  host: ""
  config:
    global:
      slack_api_url: https://hooks.slack.com/services/YOUR_KEYS_HERE
    route:
      receiver: 'default'
      repeat_interval: 1h
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerting'
        send_resolved: true
  resources:
    alertmanager:
      requests:
        cpu: 20m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 48Mi
    reloader:
      requests:
        cpu: 5m
        memory: 24Mi
      limits:
        cpu: 5m
        memory: 32Mi
    storage: 100Mi
  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: alertmanager-kubermatic
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

# ====== blackbox-exporter ======
blackboxExporter:
  image:
    repository: prom/blackbox-exporter
    tag: v0.14.0
    pullPolicy: IfNotPresent

  containers:
    blackboxExporter:
      resources:
        requests:
          cpu: 20m
          memory: 24Mi
        limits:
          cpu: 100m
          memory: 32Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: blackbox-exporter
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  modules:
    # A module that requires HTTPS and HTTP 2xx codes on its targets.
    https_2xx:
      prober: http
      timeout: 5s
      http:
        method: GET
        valid_http_versions: ["HTTP/1.1", "HTTP/2"]
        fail_if_not_ssl: true
        preferred_ip_protocol: "ip4"

# ====== grafana ======
grafana:
  user: YWRtaW4= # admin
  password: bG9vZHNlMTIz # loodse123

  image:
    repository: grafana/grafana
    tag: 6.1.3

  resources:
    requests:
      cpu: 100m
      memory: 48Mi
    limits:
      cpu: 200m
      memory: 128Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: grafana
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  # Control where the provisioning files are located.
  # If you want to *not* use the predefined ones, this
  # allows you to specify your own directory without
  # having to touch the existing files. If you only want
  # to add new elements, you can just place your YAML files
  # into the directories.
  provisioning:
    dashboards:
      source: provisioning/dashboards/*

      # You can specify additional dashboard sources inline as well.
      #extra:
      #- folder: "Initech Resources"
      #  name: "initech"
      #  options:
      #    path: /grafana-dashboard-definitions/initech
      #  org_id: 1
      #  type: file

    datasources:
      source: provisioning/datasources/*

      # You can specify additional datasources inline as well.
      #extra:
      #- name: influxdb
      #  type: influxdb
      #  access: proxy
      #  org_id: 1
      #  url: http://influxdb.monitoring.svc.cluster.local:9090
      #  version: 1
      #  editable: false

    # override Grafana configuration flags
    configuration:
      # change this to "Editor" to allow OAuth-authenticated users
      # to add and edit the dashboards
      auto_assign_org_role: Viewer

      # Set this to false if you do not have an identity-aware proxy
      # in front of Grafana and would still like to login with the
      # credentials defined above.
      disable_login_form: true

  # If you manage your dashboards via your own configmaps,
  # you can add them here to have them automatically be
  # mounted in Grafana. For each volume, specify either a
  # configMap name or a secretName, never both.
  #volumes:
  #- name: initech-public-dashboards
  #  mountPath: /initech/public-dashboards
  #  configMap: initech-dashboards-configmap
  #- name: initech-secret-dashboards
  #  mountPath: /initech/secret-dashboards
  #  secretName: initech-dashboards-secret

  # These values are injected into the dashboard JSON files,
  # see the README.md for more information on how templating
  # works.
  dashboards:
    editable: false
    transparentPanels: true
    refresh: 30s
    defaultRange: now-1h

    # Filter the namespaces shown in the Kubernetes/Volume
    # dashboard, e.g. to not show customer clusters by setting
    #
    #     namespace!~'cluster-.*'
    #
    # Do not use double quotes inside this value.
    volumeDashboardNamespaceFilter: ""
    datasourceHide: 2

# ====== kube-state-metrics ======
kubeStateMetrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.5.0
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      cpu: 200m
      memory: 128Mi

  resizer:
    image:
      repository: k8s.gcr.io/addon-resizer
      tag: '1.7'
    resources:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 48Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: kube-state-metrics
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

# ====== node-exporter ======
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.17.0
  resources:
    requests:
      cpu: 10m
      memory: 24Mi
    limits:
      cpu: 25m
      memory: 48Mi

  rbacProxy:
    image:
      repository: quay.io/coreos/kube-rbac-proxy
      tag: v0.4.1
    resources:
      requests:
        cpu: 10m
        memory: 24Mi
      limits:
        cpu: 20m
        memory: 48Mi

  nodeSelector: {}
  affinity: {}
  tolerations:
  - effect: NoExecute
    operator: Exists
  - effect: NoSchedule
    operator: Exists

# ====== prometheus ======
prometheus:
  version: v2.9.2
  host: ''
  storageSize: 100Gi

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.0.0-2
    timeout: 60m

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - config/scraping/*.yaml

    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alertmanagers:
    files:
    - config/alertmanagers/*.yaml
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to laod your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - /etc/prometheus/rules/general-*.yaml
  - /etc/prometheus/rules/kubermatic-master-*.yaml
  - /etc/prometheus/rules/kubermatic-seed-*.yaml
  # If you run in an environment where access to Kubernetes
  # scheduler and controller-manager is not possible (like GKE),
  # disable the expression below to not create false alerts
  # for missing/unhealthy components.
  - /etc/prometheus/rules/managed-*.yaml

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/2.7/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  containers:
    prometheus:
      resources:
        requests:
          cpu: 1
          memory: 2Gi
        limits:
          cpu: 2
          memory: 10Gi
    backup:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 10Gi
    reloader:
      resources:
        requests:
          cpu: 5m
          memory: 24Mi
        limits:
          cpu: 5m
          memory: 32Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: prometheus-kubermatic
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []


# =======================
# ======= Logging =======
# =======================

# ====== elasticsearch ======
logging:
  elasticsearch:
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch-oss
      tag: "6.7.1"
      pullPolicy: IfNotPresent

    cluster:
      additionalJavaOpts: ""
      config: {}
      env:
        # IMPORTANT: https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html#minimum_master_nodes
        # To prevent data loss, it is vital to configure the discovery.zen.minimum_master_nodes setting so that each master-eligible
        # node knows the minimum number of master-eligible nodes that must be visible in order to form a cluster.
        MINIMUM_MASTER_NODES: "2"

    master:
      replicas: 3
      heapSize: "256m"
      # additionalJavaOpts: "-XX:MaxRAM=512m"
      resources:
        requests:
          cpu: 75m
          memory: 350Mi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: master
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 1Gi

    data:
      replicas: 5
      heapSize: "1024m"
      # additionalJavaOpts: "-XX:MaxRAM=1536m"
      resources:
        requests:
          cpu: 200m
          memory: 1536Mi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: data
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 10Gi

    curator:
      # Amount of days after which the indicies should be killed
      interval: 5
      image:
        repository: quay.io/kubermatic/elasticsearch-curator
        tag: "5.6.0-1"
        pullPolicy: IfNotPresent

    init:
      image:
        repository: docker.io/busybox
        tag: "1.30.1"
        pullPolicy: IfNotPresent

    exporter:
      image:
        repository: justwatch/elasticsearch_exporter
        tag: "1.0.2"
        pullPolicy: IfNotPresent
      all: true
      indices: false
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 100m
          memory: 128Mi
      nodeSelector: {}
      affinity: {}
      tolerations: []

    cerebro:
      image:
        repository: yannart/cerebro
        tag: "0.8.1"
        pullPolicy: IfNotPresent
      deploy: false
      resources:
        requests:
          cpu: 100m
          memory: 400Mi
        limits:
          cpu: 200m
          # memory usage spikes when starting
          memory: 800Mi
      nodeSelector: {}
      affinity: {}
      tolerations: []

    # this key is only used to make templating easier and should
    # be removed once we switch to Elasticsearch 7.0
    resources: {}

# ====== kibana ======
  kibana:
    image:
      repository: docker.elastic.co/kibana/kibana-oss
      tag: "6.7.1"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 100m
        memory: 450Mi
      limits:
        # need more cpu upon initialization, therefore burstable class
        cpu: 1000m
        memory: 600Mi

    setupContainer:
      image:
        repository: quay.io/kubermatic/util
        tag: 1.0.0-2
        pullPolicy: IfNotPresent
      resources:
        requests:
          cpu: 10m
          memory: 24Mi
        limits:
          cpu: 10m
          memory: 32Mi

    nodeSelector: {}
    affinity: {}
    tolerations: []

# ====== fluentbit ======
  fluentbit:
    image:
      repository: fluent/fluent-bit
      tag: 1.0.6
      pullPolicy: IfNotPresent
    configuration:
      containerRuntimeParser: docker
      outputs:
      - |
        Name            es
        Match           *
        Host            ${FLUENT_ELASTICSEARCH_HOST}
        Port            ${FLUENT_ELASTICSEARCH_PORT}
        Logstash_Format On
        Retry_Limit     False
    resources:
      requests:
        cpu: 10m
        memory: 24Mi
      limits:
        cpu: 30m
        memory: 32Mi
    nodeSelector: {}
    affinity: {}
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule


# =======================
# ======= Backups =======
# =======================

# ====== velero ======
velero:
  # the Docker image for Velero;
  # if you are using restic, make sure to use an official image
  # that also contains the restic binary
  image:
    repository: gcr.io/heptio-images/velero
    tag: v0.11.0
    pullPolicy: IfNotPresent

  # CLI flags to pass to velero server; note that the two flags
  # `default-backup-storage-location` and `default-volume-snapshot-locations`
  # are automatically set via the configuration below
  serverFlags:
  - --metrics-address=:8085
  - --backup-sync-period=1m

  # whether or not to create a restic daemonset
  restic:
    deploy: true
    resources:
      requests:
        cpu: 10m
        memory: 30Mi
      limits:
        cpu: 200m
        # during backups memory usage can spike, see https://github.com/restic/restic/issues/979
        memory: 1Gi

    affinity: {}
    nodeSelector: {}
    tolerations: []

  # configure the credentials used to make snapshots (when using
  # persistentVolumeProvider) and to store backups; you can enable
  # multiple credentials, if for some reason you run on GCP and
  # still want to make restic snapshots to be stored in AWS S3.
  credentials: {}
    #aws:
    #  accessKey: ...
    #  secretKey: ...
    #gcp:
    #  serviceKey: '{...}'
    #azure:
    #  AZURE_SUBSCRIPTION_ID: ...
    #  AZURE_TENANT_ID: ...
    #  AZURE_RESOURCE_GROUP: ...
    #  AZURE_CLIENT_ID: ...
    #  AZURE_CLIENT_SECRET: ...
    #  AZURE_STORAGE_ACCOUNT_ID: ...
    #  AZURE_STORAGE_KEY: ...
    #restic:
    #  password: averysecurepassword

  # define one of your backupStorageLocations as the default
  #defaultBackupStorageLocation: aws

  # see https://heptio.github.io/velero/v0.11.0/api-types/backupstoragelocation.html
  #backupStorageLocations:
  #  aws:
  #    provider: aws
  #    objectStorage:
  #      bucket: myclusterbackups
  #    config:
  #      region: eu-west-1

  # optionally define some of your volumeSnapshotLocations as the default;
  # each element in the list must be a string of the form "provider:location"
  #defaultVolumeSnapshotLocations:
  #  - aws:aws

  # see https://heptio.github.io/velero/v0.11.0/api-types/volumesnapshotlocation.html
  #volumeSnapshotLocations:
  #  aws:
  #    provider: aws
  #    config:
  #      region: eu-west-1

  # glob expressions to find schedule defitions
  schedulesPath: schedules/*

  # Only kube2iam: change the AWS_ACCOUNT_ID and HEPTIO_VELERO_ROLE_NAME
  podAnnotations: {}
  # iam.amazonaws.com/role: arn:aws:iam::<AWS_ACCOUNT_ID>:role/<HEPTIO_VELERO_ROLE_NAME>

  resources:
    requests:
      cpu: 10m
      memory: 50Mi
    limits:
      cpu: 100m
      memory: 100Mi

  affinity:
    # Backups are potentially long-running tasks and rescheduling Velero
    # in the middle of them leaves you with broken, incomplete backups.
    # Make sure to schedule Velero on long-living, stable nodes.
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  nodeSelector: {}
  tolerations: []

