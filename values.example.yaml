# THIS FILE IS GENERATED BY https://github.com/kubermatic/kubermatic/blob/master/api/hack/ci/ci-sync-charts.sh
# ====== kubermatic ======
kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
    # the service account signing key. Must be 32 bytes or longer
    serviceAccountKey: ""
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  # The prefix for monitoring annotations in the user cluster. Default: monitoring.kubermatic.io -> monitoring.kubermatic.io/scrape, monitoring.kubermatic.io/path
  monitoringScrapeAnnotationPrefix: ""
  # The location from which to pull the Kubermatic docker image
  kubermaticImage: ""
  # The location from which to pull the Kubermatic dnatcontroller image
  dnatcontrollerImage: ""
  # The strategy to expose the cluster with, either "NodePort" which creates a NodePort with a "nodeport-proxy.k8s.io/expose": "true" annotation to expose all
  # clusters on one central Service of type LoadBalancer via the NodePort proxy or "LoadBalancer" to create a LoadBalancer service per cluster
  # **Note:** The `seed_dns_overwrite` setting of the `datacenters.yaml` doesn't have any effect if this is set to `LoadBalancer`
  exposeStrategy: "NodePort"
  # base64 encoded presets.yaml. Predefined presets for all supported providers.
  presets: ""

  # The default number of replicas for controlplane components. Can be overriden on
  # a per-cluster basis by setting .Spec.ComponentsOverride.$COMPONENT.Replicas
  apiserverDefaultReplicas: "2"
  controllerManagerDefaultReplicas: "1"
  schedulerDefaultReplicas: "1"
  maxParallelReconcile: "10"

  # Whether to disable reconciling for the apiserver endpoints
  apiserverEndpointReconcilingDisabled: false

  # Whether to load the datacenters from CRDs dynamically during runtime
  dynamicDatacenters: false

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
      helmVersion: "v2.11.0"
      image:
        repository: "quay.io/kubermatic/util"
        tag: "1.1.3"

  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"

  controller:
    # Available feature gates:
    # - OpenIDAuthPlugin
    #   If enabled configures the flags on the API server to use OAuth2 identity providers.
    # - VerticalPodAutoscaler
    #   If enabled the cluster-controller will enable the VerticalPodAutoscaler for all control plane components
    # - EtcdDataCorruptionChecks
    #   If enabled the all etcd clusters will be started with --experimental-initial-corrupt-check=true --experimental-corrupt-check-time=10m
    # For example:
    # featureGates: "OpenIDAuthPlugin=true,VerticalPodAutoscaler=true"
    featureGates: ""
    datacenterName: ""
    # Specifies the NodePort range for customer clusters - this must match the NodePort range of the seed cluster.
    nodeportRange: "30000-32767"
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.12.6-rc.1"
      pullPolicy: "IfNotPresent"
    addons:
      kubernetes:
        # list of Addons to install into every user-cluster. All need to exist in the addons image
        defaultAddons:
        - canal
        - dashboard
        - dns
        - kube-proxy
        - openvpn
        - rbac
        - kubelet-configmap
        - default-storage-class
        - node-exporter
        - nodelocal-dns-cache
        - pod-security-policy
        image:
          repository: "quay.io/kubermatic/addons"
          tag: "v2.12.6-rc.1"
          pullPolicy: "IfNotPresent"
      openshift:
        # list of Addons to install into every user-cluster. All need to exist in the addons image
        defaultAddons:
        - crd
        - openvpn
        - rbac
        - network
        - default-storage-class
        - registry
        image:
          repository: "quay.io/kubermatic/openshift-addons"
          tag: "v2.12.6-rc.1"
          pullPolicy: "IfNotPresent"
    # Specify a custom docker registry which will be used for all images (user cluster control plane + addons)
    overwriteRegistry: ""
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
    workerCount: 4
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: controller-manager
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  api:
    replicas: 2
    accessibleAddons: []
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.12.6-rc.1"
      pullPolicy: "IfNotPresent"
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 250m
        memory: 1024Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-api
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  ui:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/ui-v2"
      tag: "v2.12.5"
      pullPolicy: "IfNotPresent"
    # Config options for the dashboard:
    # default_node_count: Specify the default number of nodes.
    # share_kubeconfig: Specify if the button for "Share Kubeconfig" is visible.
    # show_demo_info: Specify if the string "Demo System" should be displayed in footer.
    # show_terms_of_service: Specify if the link to "Terms of Service" should be displayed in footer.
    # show_api_docs: Specify if link to API Docs should be displayed in footer.
    # cleanup_cluster: Specify if checkboxes for "Cleanup connected Load Balancers" &
    #                  "Cleanup connected volumes (PVs and PVCs)" should be selected by default
    #                  on cluster deletion. Users are able to deselect them via click.
    # enforce_cleanup_cluster: Enforce "Cleanup connected Load Balancers" &
    #                          "Cleanup connected volumes (PVs and PVCs)" on cluster deletion.
    #                          Checkboxes will be selected and disabled. Users are not allowed to
    #                          deselect.
    # custom_links: Specify custom links, that should be added to the menu, by defining objects with
    #               "label" and "url".
    # hide_kubernetes: Specify if button for choosing "Kubernetes" as type should be visible in wizard.
    # hide_openshift: Specify if button for choosing "OpenShift" as type should be visible in wizard.
    # oidc_provider_url: Change the base URL of the OIDC provider (BASE_URL).
    # oidc_provider_scope: Change the scope of the OIDC provider (SCOPE).
    config: |
      {
        "default_node_count": 3,
        "share_kubeconfig": false,
        "show_demo_info": false,
        "show_terms_of_service": false,
        "show_api_docs": false,
        "cleanup_cluster": false,
        "enforce_cleanup_cluster": false,
        "custom_links": [],
        "hide_kubernetes": false,
        "hide_openshift": false
      }
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 32Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-ui
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  masterController:
    replicas: 1
    image:
      repository: quay.io/kubermatic/api
      tag: "v2.12.6-rc.1"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 256Mi
    debugLog: false
    workerCount: 20
    affinity: {}
    nodeSelector: {}
    tolerations: []

  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup

  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY

  clusterNamespacePrometheus: {}
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning

  vpa:
    updater:
      image:
        repository: gcr.io/google_containers/vpa-updater
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    recommender:
      image:
        repository: gcr.io/google_containers/vpa-recommender
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 500Mi
        limits:
          cpu: 200m
          memory: 3000Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    admissioncontroller:
      image:
        repository: gcr.io/google_containers/vpa-admission-controller
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

# ====== cert-manager ======
certManager:
  isOpenshift: false

  # Optional proxy server configuration
  # http_proxy: ""
  # https_proxy: ""
  # no_proxy: ""

  controller:
    replicas: 1
    image:
      repository: quay.io/jetstack/cert-manager-controller
      tag: v0.10.1
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 100m
        memory: 30Mi
      limits:
        cpu: 300m
        memory: 50Mi

    affinity: {}
    nodeSelector: {}
    tolerations: []

    # Optional additional arguments. Use at your own risk.
    extraArgs: []
    # Must be a list of `--`-denoted args, e.G.:
    # - --foo-args=foo-value

    # Optional adddional env vars. Use at your own risk.
    extraEnv: []
    # Must be a list of valid env var definitions, e.G.:
    # - name: SOME_VAR
    #   value: 'some value'

  webhook:
    replicas: 1
    image:
      repository: quay.io/jetstack/cert-manager-webhook
      tag: v0.10.1
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 100m
        memory: 30Mi
      limits:
        cpu: 250m
        memory: 30Mi

    affinity: {}
    nodeSelector: {}
    tolerations: []

    # If true, the apiserver's cabundle will be automatically injected into the
    # webhook's ValidatingWebhookConfiguration resource by the CA injector.
    injectAPIServerCA: true

  cainjector:
    replicas: 1
    image:
      repository: quay.io/jetstack/cert-manager-cainjector
      tag: v0.10.1
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 128Mi

    nodeSelector: {}
    affinity: {}
    tolerations: []

  securityContext:
    enabled: false
    fsGroup: 1001
    runAsUser: 1001

  ingressShim: {}
    # defaultIssuerName: ""
    # defaultIssuerKind: ""
    # defaultACMEChallengeType: ""
    # defaultACMEDNS01ChallengeProvider: ""

# ====== certs ======
certificates:
  domains:
  - "kubermatic.example.com"
  - "grafana.kubermatic.example.com"
  - "prometheus.kubermatic.example.com"
  - "alertmanager.kubermatic.example.com"
  - "kibana.kubermatic.example.com"
  issuer:
    email: dev@loodse.com

  dnsValidation:
    enabled: false
    route53:
      region: ""
      accessKeyID: ""
      secretAccessKey: ""

# ====== nginx-ingress-controller ======
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3
  image:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.25.1
  config: {}
  extraArgs:
  - '--default-ssl-certificate=default/kubermatic-tls-certificates'
#    load-balance: "least_conn"
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 250m
      memory: 512Mi
  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  tolerations:
  - key: only_critical
    operator: Equal
    value: "true"
    effect: NoSchedule

  # set this to true to automatically add these tolerations
  # to make nginx run on master nodes:
  #   - { key: dedicated, operator: Equal, value: master, effect: NoSchedule }
  #   - { key: node-role.kubernetes.io/master, effect: NoSchedule }
  ignoreMasterTaint: false

# ====== nodeport-proxy ======
nodePortProxy:
  replicas: 3
  image:
    repository: "quay.io/kubermatic/nodeport-proxy"
    tag: "v2.12.6-rc.1"
  envoy:
    image:
      repository: "docker.io/envoyproxy/envoy-alpine"
      tag: v1.11.1

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: envoy
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  resources:
    envoy:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 200m
        memory: 64Mi
    envoyManager:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 150m
        memory: 48Mi
    lbUpdater:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 150m
        memory: 32Mi

  lbUpdater:
    nodeSelector: {}
    affinity: {}
    tolerations: []


  # If we're running on AWS, use an NLB. It has a fixed IP & we can use VPC endpoints
  # https://docs.aws.amazon.com/de_de/eks/latest/userguide/load-balancing.html
  service:
    annotations:
      "service.beta.kubernetes.io/aws-load-balancer-type": nlb
      # On AWS default timeout is 60s, which means: kubectl logs -f will receive EOF after 60s.
      "service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout": "3600"

# ====== oauth ======
dex:
  image:
    repository: "quay.io/dexidp/dex"
    tag: "v2.19.0"
  replicas: 2
  ingress:
    host: ""
    path: "/dex"
  expiry:
    signingKeys: "6h"
    idTokens: "24h"
#  connectors:
#  - type: github
#    id: github
#    name: GitHub
#    config:
#      clientID: some-client-id
#      clientSecret: some-client-secret
#      redirectURI: https://dev.kubermatic.io/dex/callback
#      orgs:
#      - name: kubermatic
#
#  clients:
#  - id: kubermatic
#    name: Kubermatic
#    secret: very-secret
#    RedirectURIs:
#    - http://localhost:8000
#    - https://dev.kubermatic.io
#    - http://localhost:8000/projects
#    - https://dev.kubermatic.io/projects
#
#  staticPasswordLogins:
#  - email: "admin@example.com"
#    # bcrypt hash of the string "password"
#    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
#    username: "admin"
#    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"
  resources:
    requests:
      cpu: 200m
      memory: 32Mi
    limits:
      cpu: 300m
      memory: 128Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: dex
          topologyKey: kubernetes.io/hostname
        weight: 10
  tolerations: []

# ====== minio ======
minio:
  image:
    repository: docker.io/minio/minio
    tag: RELEASE.2019-09-18T21-55-05Z
  storeSize: 100Gi
  credentials:
    accessKey: wtupllWfpMg414ZM5YkzZiUmgjh1vZdk
    secretKey: r89xkN9JvHJQppb5v7SEfkNkiC1vDcMySQFKxg6uDkE3gZfCeB7ZBfECyUOTywym

  flags:
    # Set to true to enable Minio's strict S3 compatibility mode.
    # See https://github.com/minio/minio/pull/7609 for more information.
    compat: false

    # hide sensitive information from logging
    anonymous: false

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.1.3

  # If your cluster does not have a default storage class,
  # you can specify the class to use for Minio. Note that
  # you cannot change this later on without purging the
  # chart and losing data.
  #storageClass: hdd

  resources:
    minio:
      requests:
        cpu: 100m
        memory: 32Mi
      limits:
        cpu: 300m
        memory: 512Mi
    backup:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 500m
        memory: 1500Mi

  nodeSelector: {}
  affinity: {}
  tolerations: []

# ====== iap ======
iap:
  # replicas per deployment; you can set this explicitly per deployment
  # to override this
  replicas: 2

  deployments:
    # alertmanager:
    #   name: alertmanager
    #   replicas: 2 #
    #   client_id: alertmanager
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   ## example configuration allowing access only to the mygroup from mygithuborg organization
    #     scopes:
    #     - "groups"
    #     resources:
    #     - uri: "/*"
    #       groups:
    #       - "mygithuborg:mygroup"
    #   upstream_service: alertmanager.monitoring.svc.cluster.local
    #   upstream_port: 9093
    #   ingress:
    #     host: "alertmanager.kubermatic.tld"
    #     annotations: {}
    #   # List of URL prefixes for which nginx should be configured to route requests
    #   # directly to the upstream service instead of to the Keycloak Proxy;
    #   # this can be useful for health check which should not generate load on the
    #   # identity aware proxy (Dex for example creates AuthRequest CRDs for each
    #   # opened session).
    #   # Be careful to not accidentally expose a health endpoint that in case of errors
    #   # or bad configuration can respond with confidential data (error messages,
    #   # stack traces, configuration, ...).
    #   passthrough:
    #   - /-/healthy # exposes nothing
    # grafana:
    #   name: grafana
    #   client_id: grafana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   upstream_service: grafana.monitoring.svc.cluster.local
    #   upstream_port: 3000
    #   ingress:
    #     host: "grafana.kubermatic.tld"
    #     annotations: {}
    #   passthrough:
    #   - /api/health # exposes Grafana version and Git hash
    # kibana:
    #   name: kibana
    #   client_id: kibana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   upstream_service: kibana.logging.svc.cluster.local
    #   upstream_port: 5601
    #   ingress:
    #     host: "kibana.kubermatic.tld"
    #     annotations: {}
    #   passthrough:
    #   - /ui/favicons/favicon.ico # exposes nothing
    # prometheus:
    #   name: prometheus
    #   client_id: prometheus
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   upstream_service: prometheus.monitoring.svc.cluster.local
    #   upstream_port: 9090
    #   ingress:
    #     host: "prometheus.kubermatic.tld"
    #     annotations:
    #       ingress.kubernetes.io/upstream-hash-by: "ip_hash" ## needed for prometheus federations
    #   passthrough:
    #   - /-/healthy # exposes nothing

  discovery_url: https://kubermatic.tld/dex/.well-known/openid-configuration
  port: 3000

  image:
    repository: docker.io/keycloak/keycloak-gatekeeper
    tag: 6.0.1
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 50m
      memory: 25Mi
    limits:
      cpu: 200m
      memory: 50Mi

  # You can use Go templating inside affinities and access
  # the deployment's values directly (e.g. via .name or .client_id).
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: iap
              target: '{{ .name }}'
          topologyKey: kubernetes.io/hostname
        weight: 10

  nodeSelector: {}
  tolerations: []

# ====== s3-exporter ======
s3Exporter:
  image:
    repository: quay.io/kubermatic/s3-exporter
    tag: v0.4
  endpoint: http://minio.minio.svc.cluster.local:9000
  bucket: kubermatic-etcd-backups
  resources:
    requests:
      cpu: 50m
      memory: 24Mi
    limits:
      cpu: 150m
      memory: 32Mi
  nodeSelector: {}
  affinity: {}
  tolerations: []


# ========================
# ====== Monitoring ======
# ========================

# ====== alertmanager ======
alertmanager:
  image:
    repository: quay.io/prometheus/alertmanager
    tag: v0.19.0
    pullPolicy: IfNotPresent
  configReloaderImage:
    repository: docker.io/jimmidyson/configmap-reload
    tag: v0.2.2
    pullPolicy: IfNotPresent
  host: ""
  replicas: 3
  storageSize: 100Mi
  storageClass: kubermatic-fast

  config:
    global:
      slack_api_url: https://hooks.slack.com/services/YOUR_KEYS_HERE
    route:
      receiver: default
      repeat_interval: 1h
      routes:
      - receiver: blackhole
        match:
          severity: none
    receivers:
    - name: blackhole
    - name: default
      slack_configs:
      - channel: '#alerting'
        send_resolved: true
    inhibit_rules:
    # do not alert about anything going wrong inside paused clusters
    - source_match: { alertname: KubermaticClusterPaused }
      equal: [seed_cluster, cluster]
    # if etcd is down, it brings down everything else as well
    - source_match_re: { alertname: EtcdDown, cluster: .+ }
      equal: [seed_cluster, cluster]
    # if a user-cluster apiserver is down, ignore other components failing
    - source_match_re: { alertname: KubernetesApiserverDown, cluster: .+ }
      equal: [seed_cluster, cluster]
    # if a user-cluster OpenVPN server is dead, we cannot connect to the nodes anymore
    - source_match_re: { alertname: OpenVPNServerDown, cluster: .+ }
      target_match_re: { alertname: (CAdvisorDown|KubernetesNodeDown) }
      equal: [seed_cluster, cluster]

  resources:
    alertmanager:
      requests:
        cpu: 100m
        memory: 32Mi
      limits:
        cpu: 200m
        memory: 48Mi
    reloader:
      requests:
        cpu: 50m
        memory: 24Mi
      limits:
        cpu: 150m
        memory: 32Mi
    migration:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi
  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: '{{ template "name" . }}'
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  # When upgrading the chart from 1.x => 2.0, the naming for volumes
  # has changed and without a migration your existing Alertmanager database
  # would not be used anymore. Set the enabled flag to true to let an
  # init container copy the data over. A lockfile is created so that when
  # the pod for whatever reason restarts the migration is not executed
  # again.
  # Once the migration has finished and you set the flag to false again,
  # you can safely remove the old `alertmanager-kubermatic-db-...` PVCs.
  migration:
    enabled: false
    image:
      repository: quay.io/kubermatic/util
      tag: 1.1.3

# ====== blackbox-exporter ======
blackboxExporter:
  image:
    repository: docker.io/prom/blackbox-exporter
    tag: v0.15.1
    pullPolicy: IfNotPresent

  containers:
    blackboxExporter:
      resources:
        requests:
          cpu: 100m
          memory: 24Mi
        limits:
          cpu: 250m
          memory: 32Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: blackbox-exporter
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  modules:
    # A module that requires HTTPS and HTTP 2xx codes on its targets.
    https_2xx:
      prober: http
      timeout: 5s
      http:
        method: GET
        valid_http_versions: ["HTTP/1.1", "HTTP/2"]
        fail_if_not_ssl: true
        preferred_ip_protocol: "ip4"

# ====== grafana ======
grafana:
  user: YWRtaW4= # admin
  password: bG9vZHNlMTIz # loodse123

  replicas: 1
  image:
    repository: docker.io/grafana/grafana
    tag: 6.3.5
  utilImage:
    repository: quay.io/kubermatic/util
    tag: 1.1.3
  pluginImage:
    repository: quay.io/kubermatic/grafana-plugins
    tag: 1.0.0
  resources:
    requests:
      cpu: 100m
      memory: 48Mi
    limits:
      cpu: 200m
      memory: 128Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: grafana
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  # Control where the provisioning files are located.
  # If you want to *not* use the predefined ones, this
  # allows you to specify your own directory without
  # having to touch the existing files. If you only want
  # to add new elements, you can just place your YAML files
  # into the directories.
  provisioning:
    dashboards:
      source: provisioning/dashboards/*

      # You can specify additional dashboard sources inline as well.
      #extra:
      #- folder: "Initech Resources"
      #  name: "initech"
      #  options:
      #    path: /grafana-dashboard-definitions/initech
      #  org_id: 1
      #  type: file

    datasources:
      # Kubernetes Service names to configure as data sources.
      # Names can take the form of `service[.namespace=Release.Namespace][:port=9090]`
      # and the service name itself must be unique.
      prometheusServices:
      - prometheus
      # - prometheus-thanos-query:10902

      # read datasources from this path in the Helm chart; this is
      # evaluated during deployment, not during Grafana runtime!
      source: provisioning/datasources/*

      # If you have more datasources from additional volumes,
      # specify their mount paths here to have them copied to
      # /etc/grafana/provisioning/datasources. Files will get
      # a prefix to ensure that identical filenames from
      # multiple paths do not overwrite each other.
      paths:
      - /etc/grafana/provisioning/default-datasources

      # You can specify additional datasources inline as well.
      #extra:
      #- name: influxdb
      #  type: influxdb
      #  access: proxy
      #  org_id: 1
      #  url: http://influxdb.monitoring.svc.cluster.local:9090
      #  version: 1
      #  editable: false

    # override Grafana configuration flags
    configuration:
      # change this to "Editor" to allow OAuth-authenticated users
      # to add and edit the dashboards
      auto_assign_org_role: Viewer

      # Set this to false if you do not have an identity-aware proxy
      # in front of Grafana and would still like to login with the
      # credentials defined above.
      disable_login_form: true

  # If you manage your dashboards via your own configmaps,
  # you can add them here to have them automatically be
  # mounted in Grafana. For each volume, specify either a
  # configMap name or a secretName, never both.
  #volumes:
  #- name: initech-public-dashboards
  #  mountPath: /initech/public-dashboards
  #  configMap: initech-dashboards-configmap
  #- name: initech-secret-dashboards
  #  mountPath: /initech/secret-dashboards
  #  secretName: initech-dashboards-secret

# ====== kube-state-metrics ======
kubeStateMetrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.7.2
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      cpu: 250m
      memory: 128Mi

  resizer:
    image:
      repository: gcr.io/google_containers/addon-resizer
      tag: '1.8.4' # is still the recommended version
    resources:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 48Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: kube-state-metrics
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

# ====== node-exporter ======
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.18.1
  resources:
    requests:
      cpu: 50m
      memory: 24Mi
    limits:
      cpu: 250m
      memory: 48Mi

  rbacProxy:
    image:
      repository: quay.io/coreos/kube-rbac-proxy
      tag: v0.4.1
    resources:
      requests:
        cpu: 50m
        memory: 24Mi
      limits:
        cpu: 100m
        memory: 48Mi

  nodeSelector: {}
  affinity: {}
  tolerations:
  - effect: NoExecute
    operator: Exists
  - effect: NoSchedule
    operator: Exists

# ====== prometheus ======
prometheus:
  image:
    repository: quay.io/prometheus/prometheus
    tag: v2.12.0
    pullPolicy: IfNotPresent

  host: ''
  storageSize: 100Gi
  storageClass: kubermatic-fast

  tsdb:
    retentionTime: 15d

    # This is not yet considered a truly stable
    # feature by the Prometheus developers, see
    # https://github.com/prometheus/tsdb/pull/609
    compressWAL: false

  configReloaderImage:
    repository: docker.io/jimmidyson/configmap-reload
    tag: v0.2.2
    pullPolicy: IfNotPresent

  # If you install Prometheus using a different Helm release name,
  # you can override the name used for the resources, e.g. to have
  # multiple Prometheus installed into distinct namespaces but still
  # have the same Service names, ConfigMap names etc.
  #nameOverride: prometheus

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.1.3
    timeout: 60m

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - config/scraping/*.yaml

    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alertmanagers:
    files:
    - config/alertmanagers/*.yaml
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to laod your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - /etc/prometheus/rules/general-*.yaml
  - /etc/prometheus/rules/kubermatic-master-*.yaml
  - /etc/prometheus/rules/kubermatic-seed-*.yaml
  # If you run in an environment where access to Kubernetes
  # scheduler and controller-manager is not possible (like GKE),
  # disable the expression below to not create false alerts
  # for missing/unhealthy components.
  - /etc/prometheus/rules/managed-*.yaml

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/2.7/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  # Thanos can be used to handle long-term storage of metrics by
  # shipping the data blocks into an object storage like Minio. Note that
  # this is considered EXPERIMENTAL and can be removed or significantly
  # altered in future Kubermatic releases.
  # When enabling Thanos it's advised to disable backups, as blocks
  # are already backed up, and to lower the retentionTime to something
  # short like 24 hours.
  # Enabling Thanos always disables block compaction in Prometheus and
  # enables the lifecycle and admin API.
  thanos:
    enabled: false
    image:
      repository: improbable/thanos
      tag: v0.5.0

    store:
      replicas: 2
      indexCacheSize: 500MB
      chunkPoolSize: 2GB

      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: thanos-store
                  app.kubernetes.io/instance: '{{ template "name" . }}'
              topologyKey: kubernetes.io/hostname
            weight: 100
      tolerations: []

    query:
      replicas: 2

      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: thanos-query
                  app.kubernetes.io/instance: '{{ template "name" . }}'
              topologyKey: kubernetes.io/hostname
            weight: 100
      tolerations: []

    compact:
      retention:
        # Setting any of these to "0d" disables the compaction level.
        resolutionRaw: 7d
        resolution5m: 30d
        resolution1h: 90d

      nodeSelector: {}
      affinity: {}
      tolerations: []

    # Configure the target object score according to https://thanos.io/storage.md/.
    # Make sure to manually create the target bucket.
    objstore:
      # type:
      # config:

  # When upgrading the chart from 1.x => 2.0, the naming for volumes
  # has changed and without a migration your existing Prometheus database
  # would not be used anymore. Set the enabled flag to true to let an
  # init container copy the data over. A lockfile is created so that when
  # the pod for whatever reason restarts the migration is not executed
  # again.
  # Once the migration has finished and you set the flag to false again,
  # you can safely remove the old `prometheus-kubermatic-db-...` PVCs.
  migration:
    enabled: false
    image:
      repository: quay.io/kubermatic/util
      tag: 1.1.3

  containers:
    prometheus:
      resources:
        requests:
          cpu: 1
          memory: 3Gi
        limits:
          cpu: 2
          memory: 6Gi
    backup:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 10Gi
    migration:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi
    reloader:
      resources:
        requests:
          cpu: 5m
          memory: 24Mi
        limits:
          cpu: 5m
          memory: 32Mi
    thanosSidecar:
      resources:
        requests:
          cpu: 100m
          memory: 32Mi
        limits:
          cpu: 300m
          memory: 1Gi
    thanosStore:
      resources:
        requests:
          cpu: 100m
          memory: 3Gi
        limits:
          cpu: 250m
          memory: 6Gi
    thanosQuery:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 1
          memory: 1Gi
    thanosCompact:
      resources:
        requests:
          cpu: 500m
          memory: 4Gi
        limits:
          cpu: 2
          memory: 8Gi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: '{{ template "name" . }}'
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []


# =======================
# ======= Logging =======
# =======================

# ====== elasticsearch ======
logging:
  elasticsearch:
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch-oss
      tag: "6.7.1"
      pullPolicy: IfNotPresent

    cluster:
      # make sure to always configure the JVM to have min=max heap size, or else
      # Elasticsearch will refuse to start up.
      additionalJavaOpts: "-XX:MaxRAMPercentage=70 -XX:InitialRAMPercentage=70"
      config: {}

      # environment variables used on all data and master nodes;
      # note that MINIMUM_MASTER_NODES is computed automatically and does not need
      # to be set here
      env: {}

    # When the sum of master and data nodes is 1, the chart will deploy a cluster
    # with single-node mode enabled. In this mode there is no discovery and no
    # separation between masters and data nodes anymore. You also lose any redundancy
    # and should not use this for production workloads.
    # Note that you should set data=1 and master=0 if you need a single-node cluster.
    # Scaling the StatefulSets via kubectl will *not change the mode*, so to scale
    # up you need to re-deploy the chart with updated replica counts. Once the
    # regular cluster mode is enabled, using kubectl to scale up and down is fine.

    master:
      replicas: 3
      # additionalJavaOpts: ""
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1
          memory: 1536Mi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: master
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 5Gi

    data:
      replicas: 3
      # additionalJavaOpts: ""
      resources:
        requests:
          cpu: 200m
          memory: 2560Mi
        limits:
          cpu: 1
          memory: 4Gi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: data
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 50Gi

    curator:
      # Amount of days after which the indicies should be killed
      interval: 5
      image:
        repository: quay.io/kubermatic/elasticsearch-curator
        tag: "5.7.4-1"
        pullPolicy: IfNotPresent

    init:
      image:
        repository: docker.io/library/busybox
        tag: "1.30.1"
        pullPolicy: IfNotPresent

    exporter:
      image:
        repository: docker.io/justwatch/elasticsearch_exporter
        tag: "1.1.0rc1"
        pullPolicy: IfNotPresent

      # see https://github.com/justwatchcom/elasticsearch_exporter#configuration
      all: true
      indices: true
      indices_settings: true
      shards: true

      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 100m
          memory: 128Mi

      nodeSelector: {}
      affinity: {}
      tolerations: []

    cerebro:
      image:
        repository: docker.io/lmenezes/cerebro
        tag: "0.8.3"
        pullPolicy: IfNotPresent
      deploy: false
      resources:
        requests:
          cpu: 100m
          memory: 400Mi
        limits:
          cpu: 200m
          # memory usage spikes when starting
          memory: 800Mi
      nodeSelector: {}
      affinity: {}
      tolerations: []

# ====== kibana ======
  kibana:
    image:
      repository: docker.elastic.co/kibana/kibana-oss
      tag: "6.7.1"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 100m
        memory: 450Mi
      limits:
        # need more cpu upon initialization, therefore burstable class
        cpu: 1000m
        memory: 600Mi

    setupContainer:
      image:
        repository: quay.io/kubermatic/util
        tag: 1.1.3
        pullPolicy: IfNotPresent
      resources:
        requests:
          cpu: 10m
          memory: 24Mi
        limits:
          cpu: 10m
          memory: 32Mi

    nodeSelector: {}
    affinity: {}
    tolerations: []

# ====== fluentbit ======
  fluentbit:
    image:
      repository: docker.io/fluent/fluent-bit
      tag: 1.2.2
      pullPolicy: IfNotPresent
    configuration:
      containerRuntimeParser: docker
      collectSystemd: false
      collectKernelMessages: false
      parsers: []
      outputs:
      - |
        Name            es
        Alias           elasticsearch
        Match           *
        Host            es-data
        Port            9200
        Logstash_Format On
        Replace_Dots    On
        Generate_ID     On
        Retry_Limit     False
    resources:
      requests:
        cpu: 10m
        memory: 24Mi
      limits:
        cpu: 200m
        memory: 128Mi
    nodeSelector: {}
    affinity: {}
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule


# =======================
# ======= Backups =======
# =======================

# ====== velero ======
velero:
  # the Docker image for Velero;
  # if you are using restic, make sure to use an official image
  # that also contains the restic binary
  image:
    repository: gcr.io/heptio-images/velero
    tag: v1.1.0
    pullPolicy: IfNotPresent

  # CLI flags to pass to velero server; note that the two flags
  # `default-backup-storage-location` and `default-volume-snapshot-locations`
  # are automatically set via the configuration below
  serverFlags: []

  # whether or not to create a restic daemonset
  restic:
    deploy: false
    resources:
      requests:
        cpu: 100m
        memory: 30Mi
      limits:
        cpu: 300m
        # during backups memory usage can spike, see https://github.com/restic/restic/issues/979
        memory: 1Gi

    affinity: {}
    nodeSelector: {}
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists

  # configure the credentials used to make snapshots (when using
  # persistentVolumeProvider) and to store backups; you can enable
  # multiple credentials, if for some reason you run on GCP and
  # still want to make restic snapshots to be stored in AWS S3.
  credentials: {}
    #aws:
    #  accessKey: ...
    #  secretKey: ...
    #gcp:
    #  serviceKey: '{...}'
    #azure:
    #  AZURE_SUBSCRIPTION_ID: ...
    #  AZURE_TENANT_ID: ...
    #  AZURE_RESOURCE_GROUP: ...
    #  AZURE_CLIENT_ID: ...
    #  AZURE_CLIENT_SECRET: ...
    #  AZURE_STORAGE_ACCOUNT_ID: ...
    #  AZURE_STORAGE_KEY: ...
    #restic:
    #  password: averysecurepassword

  # define one of your backupStorageLocations as the default
  #defaultBackupStorageLocation: aws

  # see https://velero.io/docs/v1.1.0/api-types/backupstoragelocation/
  #backupStorageLocations:
  #  aws:
  #    provider: aws
  #    objectStorage:
  #      bucket: myclusterbackups
  #    config:
  #      region: eu-west-1

  # optionally define some of your volumeSnapshotLocations as the default;
  # each element in the list must be a string of the form "provider:location"
  #defaultVolumeSnapshotLocations:
  #  - aws:aws

  # see https://velero.io/docs/v1.1.0/api-types/volumesnapshotlocation/
  #volumeSnapshotLocations:
  #  aws:
  #    provider: aws
  #    config:
  #      region: eu-west-1

  # glob expressions to find schedule defitions
  schedulesPath: schedules/*

  # Only kube2iam: change the AWS_ACCOUNT_ID and HEPTIO_VELERO_ROLE_NAME
  podAnnotations: {}
  # iam.amazonaws.com/role: arn:aws:iam::<AWS_ACCOUNT_ID>:role/<HEPTIO_VELERO_ROLE_NAME>

  resources:
    requests:
      cpu: 100m
      memory: 50Mi
    limits:
      cpu: 200m
      memory: 100Mi

  affinity:
    # Backups are potentially long-running tasks and rescheduling Velero
    # in the middle of them leaves you with broken, incomplete backups.
    # Make sure to schedule Velero on long-living, stable nodes.
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  nodeSelector: {}
  tolerations: []

