# THIS FILE IS GENERATED BY https://github.com/kubermatic/kubermatic/blob/master/api/hack/ci/ci-sync-charts.sh
# ====== kubermatic ======
kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
    # the service account signing key. Must be 32 bytes or longer
    serviceAccountKey: ""
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  # The prefix for monitoring annotations in the user cluster. Default: monitoring.kubermatic.io -> monitoring.kubermatic.io/scrape, monitoring.kubermatic.io/path
  monitoringScrapeAnnotationPrefix: ""
  # The location from which to pull the Kubermatic docker image
  kubermaticImage: ""
  # The location from which to pull the Kubermatic dnatcontroller image
  dnatcontrollerImage: "quay.io/kubermatic/kubeletdnat-controller"
  # The strategy to expose the cluster with, either "NodePort" which creates a NodePort with a "nodeport-proxy.k8s.io/expose": "true" annotation to expose all
  # clusters on one central Service of type LoadBalancer via the NodePort proxy or "LoadBalancer" to create a LoadBalancer service per cluster
  # **Note:** The `seed_dns_overwrite` setting of the `datacenters.yaml` doesn't have any effect if this is set to `LoadBalancer`
  exposeStrategy: "NodePort"
  # base64 encoded presets.yaml. Predefined presets for all supported providers.
  presets: ""

  # The default number of replicas for controlplane components. Can be overriden on
  # a per-cluster basis by setting .Spec.ComponentsOverride.$COMPONENT.Replicas
  apiserverDefaultReplicas: "2"
  controllerManagerDefaultReplicas: "1"
  schedulerDefaultReplicas: "1"
  maxParallelReconcile: "10"

  # Whether to disable reconciling for the apiserver endpoints
  apiserverEndpointReconcilingDisabled: false

  # Whether to load the datacenters from CRDs dynamically during runtime
  dynamicDatacenters: false

  # Whether to load the presets from CRDs dynamically during runtime
  dynamicPresets: false

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
      helmVersion: "v2.11.0"
      image:
        repository: "quay.io/kubermatic/util"
        tag: "1.3.4"

  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"

  # the cert-manager Issuer (or ClusterIssuer) responsible for managing the certificate
  certIssuer:
    name: letsencrypt-prod
    kind: ClusterIssuer

  controller:
    # Available feature gates:
    # - OpenIDAuthPlugin
    #   If enabled configures the flags on the API server to use OAuth2 identity providers.
    # - VerticalPodAutoscaler
    #   If enabled the cluster-controller will enable the VerticalPodAutoscaler for all control plane components
    # - EtcdDataCorruptionChecks
    #   If enabled the all etcd clusters will be started with --experimental-initial-corrupt-check=true --experimental-corrupt-check-time=10m
    # For example:
    # featureGates: "OpenIDAuthPlugin=true,VerticalPodAutoscaler=true"
    featureGates: ""
    datacenterName: ""
    # Specifies the NodePort range for customer clusters - this must match the NodePort range of the seed cluster.
    nodeportRange: "30000-32767"
    replicas: 2
    image:
      repository: "quay.io/kubermatic/kubermatic-ee"
      tag: "latest"
      pullPolicy: "IfNotPresent"
    pprofEndpoint: ":6600"
    addons:
      kubernetes:
        # list of Addons to install into every user-cluster. All need to exist in the addons image
        # The default list is taken from static/master/kubernetes-addons.yaml if the list below is null.
        defaultAddonsFile: kubernetes-addons.yaml
        image:
          repository: "quay.io/kubermatic/addons"
          tag: "latest"
          pullPolicy: "IfNotPresent"
      openshift:
        # Mutually exclusive with `defaultAddons`. The file referenced here is in the static/master/ folder
        defaultAddonsFile: openshift-addons.yaml
        image:
          repository: "quay.io/kubermatic/openshift-addons"
          tag: "latest"
          pullPolicy: "IfNotPresent"
    # Specify a custom docker registry which will be used for all images (user cluster control plane + addons)
    overwriteRegistry: ""
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
    workerCount: 4
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: controller-manager
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  api:
    replicas: 2
    # List of optional addons that can be installed into every user-cluster. All need to exist in the addons image.
    # The default list is taken from static/master/accessible-addons.yaml if the list below is null.
    accessibleAddons: null
    image:
      repository: "quay.io/kubermatic/kubermatic-ee"
      tag: "latest"
      pullPolicy: "IfNotPresent"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 256Mi
    pprofEndpoint: ":6600"
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-api
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  ui:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/dashboard-ee"
      tag: "master"
      pullPolicy: "IfNotPresent"
    # Config options for the dashboard, a JSON document. If this is not set, the
    # static/master/ui-config.json is used. The following options exist:
    # share_kubeconfig: Specify if the button for "Share Kubeconfig" is visible.
    # oidc_provider_url: Change the base URL of the OIDC provider (BASE_URL).
    # oidc_provider_scope: Change the scope of the OIDC provider (SCOPE).
    config: null
    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 32Mi
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          preference:
            matchExpressions:
            - key: kubermatic.io/type
              operator: In
              values:
              - stable
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                role: kubermatic-dashboard
            topologyKey: kubernetes.io/hostname
          weight: 10
    nodeSelector: {}
    tolerations:
    - key: only_critical
      operator: Equal
      value: "true"
      effect: NoSchedule

  masterController:
    replicas: 1
    image:
      repository: quay.io/kubermatic/kubermatic-ee
      tag: "latest"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 100m
        memory: 256Mi
    debugLog: false
    pprofEndpoint: ":6600"
    workerCount: 20
    affinity: {}
    nodeSelector: {}
    tolerations: []

  # You can override the default containers used for managing user cluster backups
  # using these two options. If they are left empty, the default containers from
  # the static/ directory will be used.
  # To disable backups, configure containers that just run /bin/true, for example.
  storeContainer: null
  cleanupContainer: null

  clusterNamespacePrometheus: {}
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning

  vpa:
    updater:
      image:
        repository: gcr.io/google_containers/vpa-updater
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    recommender:
      image:
        repository: gcr.io/google_containers/vpa-recommender
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 500Mi
        limits:
          cpu: 200m
          memory: 3000Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

    admissioncontroller:
      image:
        repository: gcr.io/google_containers/vpa-admission-controller
        tag: 0.5.0
      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 200m
          memory: 128Mi
      affinity: {}
      nodeSelector: {}
      tolerations: []

# ====== cert-manager ======
certManager:
  isOpenshift: false

  # Optional proxy server configuration
  # http_proxy: ""
  # https_proxy: ""
  # no_proxy: ""

  controller:
    replicas: 1
    image:
      repository: quay.io/jetstack/cert-manager-controller
      tag: v0.13.0
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 100m
        memory: 30Mi
      limits:
        cpu: 300m
        memory: 50Mi

    affinity: {}
    nodeSelector: {}
    tolerations: []

    # Optional additional arguments. Use at your own risk.
    extraArgs: []
    # Must be a list of `--`-denoted args, e.G.:
    # - --foo-args=foo-value

    # Optional adddional env vars. Use at your own risk.
    extraEnv: []
    # Must be a list of valid env var definitions, e.G.:
    # - name: SOME_VAR
    #   value: 'some value'

  webhook:
    replicas: 1
    image:
      repository: quay.io/jetstack/cert-manager-webhook
      tag: v0.13.0
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 100m
        memory: 30Mi
      limits:
        cpu: 250m
        memory: 30Mi

    affinity: {}
    nodeSelector: {}
    tolerations: []

    # If true, the apiserver's CA bundle will be automatically injected into the
    # webhook's ValidatingWebhookConfiguration resource by the CA injector.
    injectAPIServerCA: true

    # The port that the webhook should listen on for requests.
    # In GKE private clusters, by default kubernetes apiservers are allowed to
    # talk to the cluster nodes only on 443 and 10250. so configuring
    # securePort: 10250, will work out of the box without needing to add firewall
    # rules or requiring NET_BIND_SERVICE capabilities to bind port numbers <1000
    securePort: 10250

  cainjector:
    replicas: 1
    image:
      repository: quay.io/jetstack/cert-manager-cainjector
      tag: v0.13.0
      pullPolicy: IfNotPresent

    resources:
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 128Mi

    nodeSelector: {}
    affinity: {}
    tolerations: []

  securityContext:
    enabled: false
    fsGroup: 1001
    runAsUser: 1001

  ingressShim: {}
    # defaultIssuerName: ""
    # defaultIssuerKind: ""
    # defaultIssuerGroup: ""

  clusterIssuers:
    letsencrypt-prod:
      server: https://acme-v02.api.letsencrypt.org/directory
      email: dev@loodse.com
      solver:
        selector: {}
        # if DNS validation is disabled, HTTP01 validation using
        # ingresses with class=nginx will be used
        dnsValidation:
          enabled: false
          route53:
            region: ''
            accessKeyID: ''
            secretAccessKey: ''

    letsencrypt-staging:
      server: https://acme-staging-v02.api.letsencrypt.org/directory
      email: dev@loodse.com
      solver:
        selector: {}
        # if DNS validation is disabled, HTTP01 validation using
        # ingresses with class=nginx will be used
        dnsValidation:
          enabled: false
          route53:
            region: ''
            accessKeyID: ''
            secretAccessKey: ''

# ====== certs ======
certificates:
  domains:
  - "kubermatic.example.com"
  - "grafana.kubermatic.example.com"
  - "prometheus.kubermatic.example.com"
  - "alertmanager.kubermatic.example.com"
  - "kibana.kubermatic.example.com"
  issuer:
    email: dev@loodse.com

  dnsValidation:
    enabled: false
    route53:
      region: ""
      accessKeyID: ""
      secretAccessKey: ""

# ====== nginx-ingress-controller ======
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3
  image:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.29.0
  config: {}
#   load-balance: "least_conn"
  extraArgs: []
  resources:
    requests:
      cpu: 50m
      memory: 128Mi
    limits:
      cpu: 250m
      memory: 512Mi
  nodeSelector: {}
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  tolerations:
  - key: only_critical
    operator: Equal
    value: "true"
    effect: NoSchedule

  # set this to true to automatically add these tolerations
  # to make nginx run on master nodes:
  #   - { key: dedicated, operator: Equal, value: master, effect: NoSchedule }
  #   - { key: node-role.kubernetes.io/master, effect: NoSchedule }
  ignoreMasterTaint: false

  # Optionally change this to ClusterFirstWithHostNet in case you have 'hostNetwork: true'.
  # By default, while using host network, name resolution uses the host's DNS. If you wish nginx-controller
  # to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.
  dnsPolicy: ClusterFirst

# ====== nodeport-proxy ======
nodePortProxy:
  replicas: 3
  image:
    repository: "quay.io/kubermatic/nodeport-proxy"
    tag: "latest"
  envoy:
    image:
      repository: "docker.io/envoyproxy/envoy-alpine"
      tag: v1.13.0

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: envoy
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  resources:
    envoy:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 200m
        memory: 64Mi
    envoyManager:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 150m
        memory: 48Mi
    lbUpdater:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 150m
        memory: 32Mi

  lbUpdater:
    nodeSelector: {}
    affinity: {}
    tolerations: []


  # If we're running on AWS, use an NLB. It has a fixed IP & we can use VPC endpoints
  # https://docs.aws.amazon.com/de_de/eks/latest/userguide/load-balancing.html
  service:
    annotations:
      "service.beta.kubernetes.io/aws-load-balancer-type": nlb
      # On AWS default timeout is 60s, which means: kubectl logs -f will receive EOF after 60s.
      "service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout": "3600"

# ====== oauth ======
dex:
  image:
    repository: "quay.io/dexidp/dex"
    tag: "v2.22.0"
  replicas: 2
  ingress:
    # this option is required
    host: ""
    path: "/dex"
    # this option is only used for testing and should not be
    # changed to anything unencrypted in production setups
    scheme: "https"
    # if set to "non-existent", no Ingress resource will be created
    class: "nginx"
  expiry:
    signingKeys: "6h"
    idTokens: "24h"
#  connectors:
#  - type: github
#    id: github
#    name: GitHub
#    config:
#      clientID: some-client-id
#      clientSecret: some-client-secret
#      redirectURI: https://dev.kubermatic.io/dex/callback
#      orgs:
#      - name: kubermatic
#
#  clients:
#  - id: kubermatic
#    name: Kubermatic
#    secret: very-secret
#    RedirectURIs:
#    - http://localhost:8000
#    - https://dev.kubermatic.io
#    - http://localhost:8000/projects
#    - https://dev.kubermatic.io/projects
#
#  staticPasswords:
#  - email: "admin@example.com"
#    # bcrypt hash of the string "password"
#    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
#    username: "admin"
#    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"
  resources:
    requests:
      cpu: 200m
      memory: 32Mi
    limits:
      cpu: 300m
      memory: 128Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: dex
          topologyKey: kubernetes.io/hostname
        weight: 10
  tolerations: []

  # the cert-manager Issuer (or ClusterIssuer) responsible for managing the certificates
  certIssuer:
    name: letsencrypt-prod
    kind: ClusterIssuer

# ====== minio ======
minio:
  image:
    repository: docker.io/minio/minio
    tag: RELEASE.2019-10-12T01-39-57Z
  storeSize: 100Gi
  credentials:
    accessKey: wtupllWfpMg414ZM5YkzZiUmgjh1vZdk
    secretKey: r89xkN9JvHJQppb5v7SEfkNkiC1vDcMySQFKxg6uDkE3gZfCeB7ZBfECyUOTywym

  flags:
    # Set to true to enable Minio's strict S3 compatibility mode.
    # See https://github.com/minio/minio/pull/7609 for more information.
    compat: false

    # hide sensitive information from logging
    anonymous: false

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.3.4

  # If your cluster does not have a default storage class,
  # you can specify the class to use for Minio. Note that
  # you cannot change this later on without purging the
  # chart and losing data.
  #storageClass: hdd

  resources:
    minio:
      requests:
        cpu: 100m
        memory: 32Mi
      limits:
        cpu: 300m
        memory: 512Mi
    backup:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 500m
        memory: 1500Mi

  nodeSelector: {}
  affinity: {}
  tolerations: []

# ====== iap ======
iap:
  # replicas per deployment; you can set this explicitly per deployment
  # to override this
  replicas: 2

  deployments:
    # alertmanager:
    #   name: alertmanager
    #   replicas: 2 #
    #   client_id: alertmanager
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   ## example configuration allowing access only to the mygroup from mygithuborg organization
    #     scopes:
    #     - "groups"
    #     resources:
    #     - uri: "/*"
    #       groups:
    #       - "mygithuborg:mygroup"
    #   upstream_service: alertmanager.monitoring.svc.cluster.local
    #   upstream_port: 9093
    #   ingress:
    #     host: "alertmanager.kubermatic.tld"
    #     annotations: {}
    #   # List of URL prefixes for which nginx should be configured to route requests
    #   # directly to the upstream service instead of to the Keycloak Proxy;
    #   # this can be useful for health check which should not generate load on the
    #   # identity aware proxy (Dex for example creates AuthRequest CRDs for each
    #   # opened session).
    #   # Be careful to not accidentally expose a health endpoint that in case of errors
    #   # or bad configuration can respond with confidential data (error messages,
    #   # stack traces, configuration, ...).
    #   passthrough:
    #   - /-/healthy # exposes nothing
    # grafana:
    #   name: grafana
    #   client_id: grafana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   upstream_service: grafana.monitoring.svc.cluster.local
    #   upstream_port: 3000
    #   ingress:
    #     host: "grafana.kubermatic.tld"
    #     annotations: {}
    #   passthrough:
    #   - /api/health # exposes Grafana version and Git hash
    # kibana:
    #   name: kibana
    #   client_id: kibana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   upstream_service: kibana.logging.svc.cluster.local
    #   upstream_port: 5601
    #   ingress:
    #     host: "kibana.kubermatic.tld"
    #     annotations: {}
    #   passthrough:
    #   - /ui/favicons/favicon.ico # exposes nothing
    # prometheus:
    #   name: prometheus
    #   client_id: prometheus
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://www.keycloak.org/docs/latest/securing_apps/index.html#example-usage-and-configuration
    #   upstream_service: prometheus.monitoring.svc.cluster.local
    #   upstream_port: 9090
    #   ingress:
    #     host: "prometheus.kubermatic.tld"
    #     annotations:
    #       ingress.kubernetes.io/upstream-hash-by: "ip_hash" ## needed for prometheus federations
    #   passthrough:
    #   - /-/healthy # exposes nothing

  # the cert-manager Issuer (or ClusterIssuer) responsible for managing the certificates
  certIssuer:
    name: letsencrypt-prod
    kind: ClusterIssuer

  discovery_url: https://kubermatic.tld/dex/.well-known/openid-configuration
  port: 3000

  image:
    repository: docker.io/keycloak/keycloak-gatekeeper
    tag: 7.0.0
    pullPolicy: IfNotPresent

  resources:
    requests:
      cpu: 50m
      memory: 25Mi
    limits:
      cpu: 200m
      memory: 50Mi

  # You can use Go templating inside affinities and access
  # the deployment's values directly (e.g. via .name or .client_id).
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: iap
              target: '{{ .name }}'
          topologyKey: kubernetes.io/hostname
        weight: 10

  nodeSelector: {}
  tolerations: []

# ====== s3-exporter ======
s3Exporter:
  image:
    repository: quay.io/kubermatic/s3-exporter
    tag: v0.4
  endpoint: http://minio.minio.svc.cluster.local:9000
  bucket: kubermatic-etcd-backups
  resources:
    requests:
      cpu: 50m
      memory: 24Mi
    limits:
      cpu: 150m
      memory: 32Mi
  nodeSelector: {}
  affinity: {}
  tolerations: []


# ========================
# ====== Monitoring ======
# ========================

# ====== alertmanager ======
alertmanager:
  image:
    repository: quay.io/prometheus/alertmanager
    tag: v0.20.0
    pullPolicy: IfNotPresent
  configReloaderImage:
    repository: docker.io/jimmidyson/configmap-reload
    tag: v0.2.2
    pullPolicy: IfNotPresent
  host: ""
  replicas: 3
  storageSize: 100Mi
  storageClass: kubermatic-fast

  config:
    global:
      slack_api_url: https://hooks.slack.com/services/YOUR_KEYS_HERE
    route:
      receiver: default
      repeat_interval: 1h
      routes:
      - receiver: blackhole
        match:
          severity: none
    receivers:
    - name: blackhole
    - name: default
      slack_configs:
      - channel: '#alerting'
        send_resolved: true
    inhibit_rules:
    # do not alert about anything going wrong inside paused clusters
    - source_match: { alertname: KubermaticClusterPaused }
      equal: [seed_cluster, cluster]
    # if etcd is down, it brings down everything else as well
    - source_match_re: { alertname: EtcdDown, cluster: .+ }
      equal: [seed_cluster, cluster]
    # if a user-cluster apiserver is down, ignore other components failing
    - source_match_re: { alertname: KubernetesApiserverDown, cluster: .+ }
      equal: [seed_cluster, cluster]
    # if a user-cluster OpenVPN server is dead, we cannot connect to the nodes anymore
    - source_match_re: { alertname: OpenVPNServerDown, cluster: .+ }
      target_match_re: { alertname: (CAdvisorDown|KubernetesNodeDown) }
      equal: [seed_cluster, cluster]

  resources:
    alertmanager:
      requests:
        cpu: 100m
        memory: 32Mi
      limits:
        cpu: 200m
        memory: 48Mi
    reloader:
      requests:
        cpu: 50m
        memory: 24Mi
      limits:
        cpu: 150m
        memory: 32Mi
    migration:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi
  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: '{{ template "name" . }}'
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  # When upgrading the chart from 1.x => 2.0, the naming for volumes
  # has changed and without a migration your existing Alertmanager database
  # would not be used anymore. Set the enabled flag to true to let an
  # init container copy the data over. A lockfile is created so that when
  # the pod for whatever reason restarts the migration is not executed
  # again.
  # Once the migration has finished and you set the flag to false again,
  # you can safely remove the old `alertmanager-kubermatic-db-...` PVCs.
  migration:
    enabled: false
    image:
      repository: quay.io/kubermatic/util
      tag: 1.3.4

# ====== blackbox-exporter ======
blackboxExporter:
  image:
    repository: docker.io/prom/blackbox-exporter
    tag: v0.16.0
    pullPolicy: IfNotPresent

  containers:
    blackboxExporter:
      resources:
        requests:
          cpu: 100m
          memory: 24Mi
        limits:
          cpu: 250m
          memory: 32Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: blackbox-exporter
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  modules:
    # A module that requires HTTPS and HTTP 2xx codes on its targets.
    https_2xx:
      prober: http
      timeout: 5s
      http:
        method: GET
        valid_http_versions: ["HTTP/1.1", "HTTP/2"]
        fail_if_not_ssl: true
        preferred_ip_protocol: "ip4"

# ====== grafana ======
grafana:
  user: YWRtaW4= # admin
  password: bG9vZHNlMTIz # loodse123

  replicas: 1
  image:
    repository: docker.io/grafana/grafana
    tag: 6.7.1
  utilImage:
    repository: quay.io/kubermatic/util
    tag: 1.3.4
  pluginImage:
    repository: quay.io/kubermatic/grafana-plugins
    tag: 1.0.0
  resources:
    requests:
      cpu: 100m
      memory: 48Mi
    limits:
      cpu: 200m
      memory: 128Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: grafana
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

  # Control where the provisioning files are located.
  # If you want to *not* use the predefined ones, this
  # allows you to specify your own directory without
  # having to touch the existing files. If you only want
  # to add new elements, you can just place your YAML files
  # into the directories.
  provisioning:
    dashboards:
      source: provisioning/dashboards/*

      # You can specify additional dashboard sources inline as well.
      #extra:
      #- folder: "Initech Resources"
      #  name: "initech"
      #  options:
      #    path: /grafana-dashboard-definitions/initech
      #  org_id: 1
      #  type: file

    datasources:
      # Kubernetes Service names to configure as data sources.
      # Names can take the form of `service[.namespace=Release.Namespace][:port=9090]`
      # and the service name itself must be unique.
      prometheusServices:
      - prometheus
      # - prometheus-thanos-query:10902

      lokiServices: []
      # - loki
      
      # read datasources from this path in the Helm chart; this is
      # evaluated during deployment, not during Grafana runtime!
      source: provisioning/datasources/*

      # If you have more datasources from additional volumes,
      # specify their mount paths here to have them copied to
      # /etc/grafana/provisioning/datasources. Files will get
      # a prefix to ensure that identical filenames from
      # multiple paths do not overwrite each other.
      paths:
      - /etc/grafana/provisioning/default-datasources

      # You can specify additional datasources inline as well.
      #extra:
      #- name: influxdb
      #  type: influxdb
      #  access: proxy
      #  org_id: 1
      #  url: http://influxdb.monitoring.svc.cluster.local:9090
      #  version: 1
      #  editable: false

    # override Grafana configuration flags
    configuration:
      # change this to "Editor" to allow OAuth-authenticated users
      # to add and edit the dashboards
      auto_assign_org_role: Viewer

      # Set this to false if you do not have an identity-aware proxy
      # in front of Grafana and would still like to login with the
      # credentials defined above.
      disable_login_form: true

      # Set this to true to allow Viewers to
      # edit existing dashboards and explore datasources.
      # Viewers will still not be able to save these changes back
      # but they will be able to see other metrics than exposed by
      # the dashboards.
      viewers_can_edit: false

  # If you manage your dashboards via your own configmaps,
  # you can add them here to have them automatically be
  # mounted in Grafana. For each volume, specify either a
  # configMap name or a secretName, never both.
  #volumes:
  #- name: initech-public-dashboards
  #  mountPath: /initech/public-dashboards
  #  configMap: initech-dashboards-configmap
  #- name: initech-secret-dashboards
  #  mountPath: /initech/secret-dashboards
  #  secretName: initech-dashboards-secret

# ====== kube-state-metrics ======
kubeStateMetrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.9.4
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      cpu: 250m
      memory: 128Mi

  resizer:
    image:
      repository: gcr.io/google_containers/addon-resizer
      tag: '1.8.4' # is still the recommended version
    resources:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 48Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: kube-state-metrics
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []

# ====== node-exporter ======
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.18.1
  resources:
    requests:
      cpu: 50m
      memory: 24Mi
    limits:
      cpu: 250m
      memory: 48Mi

  rbacProxy:
    image:
      repository: quay.io/coreos/kube-rbac-proxy
      tag: v0.4.1
    resources:
      requests:
        cpu: 50m
        memory: 24Mi
      limits:
        cpu: 100m
        memory: 48Mi

  nodeSelector: {}
  affinity: {}
  tolerations:
  - effect: NoExecute
    operator: Exists
  - effect: NoSchedule
    operator: Exists

# ====== prometheus ======
prometheus:
  image:
    repository: quay.io/prometheus/prometheus
    tag: v2.17.0
    pullPolicy: IfNotPresent

  host: ''
  storageSize: 100Gi
  storageClass: kubermatic-fast

  tsdb:
    retentionTime: 15d

    # This is not yet considered a truly stable
    # feature by the Prometheus developers, see
    # https://github.com/prometheus/tsdb/pull/609
    compressWAL: false

  configReloaderImage:
    repository: docker.io/jimmidyson/configmap-reload
    tag: v0.3.0
    pullPolicy: IfNotPresent

  # If you install Prometheus using a different Helm release name,
  # you can override the name used for the resources, e.g. to have
  # multiple Prometheus installed into distinct namespaces but still
  # have the same Service names, ConfigMap names etc.
  #nameOverride: prometheus

  backup:
    enabled: true
    image:
      repository: quay.io/kubermatic/util
      tag: 1.3.4
    timeout: 60m

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - config/scraping/*.yaml

    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alertmanagers:
    files:
    - config/alertmanagers/*.yaml
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to laod your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - /etc/prometheus/rules/general-*.yaml
  - /etc/prometheus/rules/kubermatic-seed-*.yaml
  # I you are running a non-master cluster, you should comment the
  # following line to disable master components alerts.
  - /etc/prometheus/rules/kubermatic-master-*.yaml
  # If you run in an environment where access to Kubernetes
  # scheduler and controller-manager is not possible (like GKE),
  # disable the expression below to not create false alerts
  # for missing/unhealthy components.
  - /etc/prometheus/rules/managed-*.yaml

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/2.7/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  # Thanos can be used to handle long-term storage of metrics by
  # shipping the data blocks into an object storage like Minio. Note that
  # this is considered EXPERIMENTAL and can be removed or significantly
  # altered in future Kubermatic releases.
  # When enabling Thanos it's advised to disable backups, as blocks
  # are already backed up, and to lower the retentionTime to something
  # short like 24 hours.
  # Enabling Thanos always disables block compaction in Prometheus and
  # enables the lifecycle and admin API.
  thanos:
    enabled: false
    image:
      repository: quay.io/thanos/thanos
      tag: v0.11.0

    store:
      replicas: 2
      indexCacheSize: 500MB
      chunkPoolSize: 2GB

      # Thanos Store downloads blocks from the storage to create its local
      # cache before enabling the readiness/liveness endpoints, see
      # https://github.com/thanos-io/thanos/pull/1460
      # This means that pods can stay un-ready for quite some time, which
      # can slow down rollouts significantly. By default the probes for
      # Thanos Store are therefore disabled, but you can re-enable them
      # by setting probeDelaySeconds to any value >= 0.
      probeDelaySeconds: -1

      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: thanos-store
                  app.kubernetes.io/instance: '{{ template "name" . }}'
              topologyKey: kubernetes.io/hostname
            weight: 100
      tolerations: []

    query:
      replicas: 2

      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: thanos-query
                  app.kubernetes.io/instance: '{{ template "name" . }}'
              topologyKey: kubernetes.io/hostname
            weight: 100
      tolerations: []

    compact:
      retention:
        # Setting any of these to "0d" disables the compaction level.
        resolutionRaw: 7d
        resolution5m: 30d
        resolution1h: 90d

      nodeSelector: {}
      affinity: {}
      tolerations: []

    ui:
      replicas: 2
      nodeSelector: {}
      affinity: {}
      tolerations: []

    # Configure the target object score according to https://thanos.io/storage.md/.
    # Make sure to manually create the target bucket.
    objstore:
      # type:
      # config:

  # When upgrading the chart from 1.x => 2.0, the naming for volumes
  # has changed and without a migration your existing Prometheus database
  # would not be used anymore. Set the enabled flag to true to let an
  # init container copy the data over. A lockfile is created so that when
  # the pod for whatever reason restarts the migration is not executed
  # again.
  # Once the migration has finished and you set the flag to false again,
  # you can safely remove the old `prometheus-kubermatic-db-...` PVCs.
  migration:
    enabled: false
    image:
      repository: quay.io/kubermatic/util
      tag: 1.3.4

  containers:
    prometheus:
      resources:
        requests:
          cpu: 1
          memory: 3Gi
        limits:
          cpu: 2
          memory: 6Gi
    backup:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 500m
          memory: 10Gi
    migration:
      resources:
        requests:
          cpu: 100m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 128Mi
    reloader:
      resources:
        requests:
          cpu: 5m
          memory: 24Mi
        limits:
          cpu: 5m
          memory: 32Mi
    thanosSidecar:
      resources:
        requests:
          cpu: 100m
          memory: 32Mi
        limits:
          cpu: 300m
          memory: 1Gi
    thanosStore:
      resources:
        requests:
          cpu: 250m
          memory: 1536Mi
        limits:
          cpu: 1
          memory: 3Gi
    thanosQuery:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 1
          memory: 1Gi
    thanosCompact:
      resources:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          cpu: 2
          memory: 4Gi
    thanosUI:
      resources:
        requests:
          cpu: 10m
          memory: 32Mi
        limits:
          cpu: 250m
          memory: 128Mi

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              app: '{{ template "name" . }}'
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations: []


# =======================
# ======= Logging =======
# =======================

# ====== loki ======
loki:
  affinity: {}
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - loki
  #     topologyKey: "kubernetes.io/hostname"
  
  # enable tracing for debug, need install jaeger and specify right jaeger_agent_host
  tracing:
    jaegerAgentHost:
  
  config:
    auth_enabled: false
    ingester:
      chunk_idle_period: 3m
      chunk_block_size: 262144
      chunk_retain_period: 1m
      lifecycler:
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
  
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 144h
    schema_config:
      configs:
      - from: 2018-04-15
        store: boltdb
        object_store: filesystem
        schema: v9
        index:
          prefix: index_
          period: 144h
    server:
      http_listen_port: 3100
    storage_config:
      boltdb:
        directory: /data/loki/index
      filesystem:
        directory: /data/loki/chunks
    chunk_store_config:
      max_look_back_period: 720h
    table_manager:
      retention_deletes_enabled: true
      retention_period: 720h
  
  image:
    repository: docker.io/grafana/loki
    tag: v1.3.0
    pullPolicy: IfNotPresent
  
  ## The app name of loki clients
  client: {}
    # name:
  
  nodeSelector: {}
  
  persistence:
    size: 15Gi
    storageClassName: kubermatic-fast
  
  podLabels: {}
  
  replicas: 1
  
  resources: 
   limits:
     cpu: "1"
     memory: 700Mi
   requests:
     cpu: 300m
     memory: 256Mi
  
  service:
    type: ClusterIP
    nodePort:
    port: 3100
    annotations: {}
    labels: {}
  
  tolerations: []
  
  updateStrategy:
    type: RollingUpdate

# ====== promtail ======
promtail:
  deploymentStrategy: RollingUpdate
  
  image:
    repository: docker.io/grafana/promtail
    tag: v1.3.0
    pullPolicy: IfNotPresent
  
  loki:
    serviceName: "loki"  
    servicePort: 3100
    serviceScheme: http
  
  nodeSelector: {}
  affinity: {}
  
  resources: 
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
  
  scrapeConfigs:
    - job_name: kubernetes-pods-name
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels:
        - __meta_kubernetes_pod_label_name
        target_label: __service__
      - source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: ''
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: instance
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_container_name
        target_label: container_name
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_uid
        - __meta_kubernetes_pod_container_name
        target_label: __path__
    - job_name: kubernetes-pods-app
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: drop
        regex: .+
        source_labels:
        - __meta_kubernetes_pod_label_name
      - source_labels:
        - __meta_kubernetes_pod_label_app
        target_label: __service__
      - source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: ''
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: instance
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_container_name
        target_label: container_name
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_uid
        - __meta_kubernetes_pod_container_name
        target_label: __path__
    - job_name: kubernetes-pods-direct-controllers
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: drop
        regex: .+
        separator: ''
        source_labels:
        - __meta_kubernetes_pod_label_name
        - __meta_kubernetes_pod_label_app
      - action: drop
        regex: '[0-9a-z-.]+-[0-9a-f]{8,10}'
        source_labels:
        - __meta_kubernetes_pod_controller_name
      - source_labels:
        - __meta_kubernetes_pod_controller_name
        target_label: __service__
      - source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: ''
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: instance
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_container_name
        target_label: container_name
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_uid
        - __meta_kubernetes_pod_container_name
        target_label: __path__
    - job_name: kubernetes-pods-indirect-controller
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: drop
        regex: .+
        separator: ''
        source_labels:
        - __meta_kubernetes_pod_label_name
        - __meta_kubernetes_pod_label_app
      - action: keep
        regex: '[0-9a-z-.]+-[0-9a-f]{8,10}'
        source_labels:
        - __meta_kubernetes_pod_controller_name
      - action: replace
        regex: '([0-9a-z-.]+)-[0-9a-f]{8,10}'
        source_labels:
        - __meta_kubernetes_pod_controller_name
        target_label: __service__
      - source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: ''
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: instance
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_container_name
        target_label: container_name
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_uid
        - __meta_kubernetes_pod_container_name
        target_label: __path__
    - job_name: kubernetes-pods-static
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: drop
        regex: ''
        source_labels:
        - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_label_component
        target_label: __service__
      - source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: drop
        regex: ''
        source_labels:
        - __service__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        replacement: $1
        separator: /
        source_labels:
        - __meta_kubernetes_namespace
        - __service__
        target_label: job
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: instance
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_container_name
        target_label: container_name
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror
        - __meta_kubernetes_pod_container_name
        target_label: __path__

  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
  
  config:
    client:
      # Maximum wait period before sending batch
      batchwait: 1s
      # Maximum batch size to accrue before sending, unit is byte
      batchsize: 102400
  
      # Maximum time to wait for server to respond to a request
      timeout: 10s
  
      backoff_config:
        # Initial backoff time between retries
        minbackoff: 100ms
        # Maximum backoff time between retries
        maxbackoff: 5s
        # Maximum number of retries when sending batches, 0 means infinite retries
        maxretries: 20
  
      # The labels to add to any time series or alerts when communicating with loki
      external_labels: {}
  
    server:
      http_listen_port: 3101
    positions:
      filename: /run/promtail/positions.yaml
    target_config:
      # Period to resync directories being watched and files being tailed
      sync_period: 10s
  

# ====== elasticsearch ======
logging:
  elasticsearch:
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch-oss
      tag: "6.8.5"
      pullPolicy: IfNotPresent

    cluster:
      # make sure to always configure the JVM to have min=max heap size, or else
      # Elasticsearch will refuse to start up.
      additionalJavaOpts: "-XX:MaxRAMPercentage=70 -XX:InitialRAMPercentage=70"
      config: {}

      # environment variables used on all data and master nodes;
      # note that MINIMUM_MASTER_NODES is computed automatically and does not need
      # to be set here
      env: {}

    # When the sum of master and data nodes is 1, the chart will deploy a cluster
    # with single-node mode enabled. In this mode there is no discovery and no
    # separation between masters and data nodes anymore. You also lose any redundancy
    # and should not use this for production workloads.
    # Note that you should set data=1 and master=0 if you need a single-node cluster.
    # Scaling the StatefulSets via kubectl will *not change the mode*, so to scale
    # up you need to re-deploy the chart with updated replica counts. Once the
    # regular cluster mode is enabled, using kubectl to scale up and down is fine.

    master:
      replicas: 3
      # additionalJavaOpts: ""
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1
          memory: 1536Mi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: master
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 5Gi

    data:
      replicas: 3
      # additionalJavaOpts: ""
      resources:
        requests:
          cpu: 200m
          memory: 2560Mi
        limits:
          cpu: 1
          memory: 4Gi
      nodeSelector: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  component: elasticsearch
                  role: data
              topologyKey: kubernetes.io/hostname
      tolerations: []
      storageSize: 50Gi

    curator:
      # Amount of days after which the indicies should be killed
      interval: 5
      image:
        repository: quay.io/kubermatic/elasticsearch-curator
        tag: "5.7.4-1"
        pullPolicy: IfNotPresent

    init:
      image:
        repository: docker.io/library/busybox
        tag: "1.30.1"
        pullPolicy: IfNotPresent

    exporter:
      image:
        repository: docker.io/justwatch/elasticsearch_exporter
        tag: "1.1.0rc1"
        pullPolicy: IfNotPresent

      # see https://github.com/justwatchcom/elasticsearch_exporter#configuration
      all: true
      indices: true
      indices_settings: true
      shards: true

      resources:
        requests:
          cpu: 50m
          memory: 32Mi
        limits:
          cpu: 100m
          memory: 128Mi

      nodeSelector: {}
      affinity: {}
      tolerations: []

    cerebro:
      image:
        repository: docker.io/lmenezes/cerebro
        tag: "0.8.3"
        pullPolicy: IfNotPresent
      deploy: false
      resources:
        requests:
          cpu: 100m
          memory: 400Mi
        limits:
          cpu: 200m
          # memory usage spikes when starting
          memory: 800Mi
      nodeSelector: {}
      affinity: {}
      tolerations: []

# ====== kibana ======
  kibana:
    image:
      repository: docker.elastic.co/kibana/kibana-oss
      tag: "6.8.5"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 100m
        memory: 450Mi
      limits:
        # need more cpu upon initialization, therefore burstable class
        cpu: 1000m
        memory: 600Mi

    setupContainer:
      image:
        repository: quay.io/kubermatic/util
        tag: 1.3.4
        pullPolicy: IfNotPresent
      resources:
        requests:
          cpu: 10m
          memory: 24Mi
        limits:
          cpu: 10m
          memory: 32Mi

    nodeSelector: {}
    affinity: {}
    tolerations: []

# ====== fluentbit ======
  fluentbit:
    image:
      repository: docker.io/fluent/fluent-bit
      tag: 1.2.2
      pullPolicy: IfNotPresent
    configuration:
      containerRuntimeParser: docker
      collectSystemd: false
      collectKernelMessages: false
      parsers: []
      outputs:
      - |
        Name            es
        Alias           elasticsearch
        Match           *
        Host            es-data
        Port            9200
        Logstash_Format On
        Replace_Dots    On
        Generate_ID     On
        Retry_Limit     10
    resources:
      requests:
        cpu: 10m
        memory: 24Mi
      limits:
        cpu: 200m
        memory: 128Mi
    nodeSelector: {}
    affinity: {}
    tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule


# =======================
# ======= Backups =======
# =======================

# ====== velero ======
velero:
  # the Docker image for Velero;
  # if you are using restic, make sure to use an official image
  # that also contains the restic binary
  image:
    repository: docker.io/velero/velero
    tag: v1.3.2
    pullPolicy: IfNotPresent

  # CLI flags to pass to velero server; note that the two flags
  # `default-backup-storage-location` and `default-volume-snapshot-locations`
  # are automatically set via the configuration below
  serverFlags: []

  # whether or not to create a restic daemonset
  restic:
    deploy: false
    resources:
      requests:
        cpu: 100m
        memory: 30Mi
      limits:
        cpu: 300m
        # during backups memory usage can spike, see https://github.com/restic/restic/issues/979
        memory: 1Gi

    affinity: {}
    nodeSelector: {}
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists

  # configure the credentials used to make snapshots (when using
  # persistentVolumeProvider) and to store backups; you can enable
  # multiple credentials, if for some reason you run on GCP and
  # still want to make restic snapshots to be stored in AWS S3.
  credentials: {}
    #aws:
    #  accessKey: ...
    #  secretKey: ...
    #gcp:
    #  serviceKey: '{...}'
    #azure:
    #  AZURE_SUBSCRIPTION_ID: ...
    #  AZURE_TENANT_ID: ...
    #  AZURE_RESOURCE_GROUP: ...
    #  AZURE_CLIENT_ID: ...
    #  AZURE_CLIENT_SECRET: ...
    #  AZURE_STORAGE_ACCOUNT_ID: ...
    #  AZURE_STORAGE_KEY: ...
    #restic:
    #  password: averysecurepassword

  # define one of your backupStorageLocations as the default
  #defaultBackupStorageLocation: aws

  # see https://velero.io/docs/v1.1.0/api-types/backupstoragelocation/
  #backupStorageLocations:
  #  aws:
  #    provider: aws
  #    objectStorage:
  #      bucket: myclusterbackups
  #    config:
  #      region: eu-west-1

  # optionally define some of your volumeSnapshotLocations as the default;
  # each element in the list must be a string of the form "provider:location"
  #defaultVolumeSnapshotLocations:
  #  - aws:aws

  # see https://velero.io/docs/v1.1.0/api-types/volumesnapshotlocation/
  #volumeSnapshotLocations:
  #  aws:
  #    provider: aws
  #    config:
  #      region: eu-west-1

  # glob expressions to find schedule defitions
  schedulesPath: schedules/*

  # Only kube2iam: change the AWS_ACCOUNT_ID and HEPTIO_VELERO_ROLE_NAME
  podAnnotations: {}
  # iam.amazonaws.com/role: arn:aws:iam::<AWS_ACCOUNT_ID>:role/<HEPTIO_VELERO_ROLE_NAME>

  resources:
    requests:
      cpu: 100m
      memory: 50Mi
    limits:
      cpu: 200m
      memory: 100Mi

  affinity:
    # Backups are potentially long-running tasks and rescheduling Velero
    # in the middle of them leaves you with broken, incomplete backups.
    # Make sure to schedule Velero on long-living, stable nodes.
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: kubermatic.io/type
            operator: In
            values:
            - stable
  nodeSelector: {}
  tolerations: []

